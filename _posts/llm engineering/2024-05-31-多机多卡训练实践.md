---
layout: post
title: 模型的多机多卡训练实践
date: 2024-05-31
tags: [深度学习, 分布式训练, 多机多卡]
categories: llm engineering
---

# 模型的多机多卡训练实践
以下是在多个模型框架上进行多机多卡、单机多卡训练的记录。

## 多机多卡训练问题排查
常见问题

1. dataset和dataloader实现是否正常

可以在本地先遍历读取，观察是否存在问题，尤其是dataloader在多个num_workers的情况下可能存在问题，可能有的错误为
- 数据集预处理逻辑在多 worker 环境下存在 race condition，导致 __getitem__ 返回的 tensor 维度不一致（1D/2D 混杂）。
- 自定义 collate_fn 未对 batch 内样本长度做 padding 统一，直接堆叠导致异常。

2. 模型训练时的参数是分片在多个机器上的，训练保存默认merge所有参数导致nccl timeout

这个问题有`可能`出现在deepspeed zero3模式下，因为 `transformers + accelerate`训练默认会保存.safetensors模型，这个需要将deepspeed分片的参数merge，之后在更新到safetensors模型中；
对于参数不做分片的训练配置下，大概率不会出现问题

3. 基础的NCCL配置

这里需要根据各自的网络配置、集群拓扑（GPU/网卡/交换机、节点数）和当前每步耗时构成，设置相对最优的配置，

## 多机多卡（以megatts2训练代码为例）
这里的代码框架是基于pylightning的

### 1. **训练环境启动(docker)**

```bash
# 主机，从机都要创建训练容器，命名可以不同
# FROM nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04, cuda12.1
docker run -it --gpus all -v /mnt:/mnt --shm-size="100g" -d --network host --pid host --name <your_container> tts-test /bin/bash
# 进入容器, 更新lightning
# 解决 strict_loading问题
pip3 install lightning==2.2.1 -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2. **确认主机，从机在相同的网络ID下**

运行 ifconfig, 查看网卡(bond/)上对应inet，结合netmask查看网络ID，没有ifconfig， 安装一下: apt-get install net-tools

如果不在，参考[docker容器中deepspeed多机多卡集群分布式训练大模型 - 掘金](https://juejin.cn/post/7304182487683923994)创建overlay共享网络

```
ainode02: 172.16.12.2
ainode08: 172.16.12.8
```

### 3. **确认NCCL是否安装**

lightning默认使用NCCL实现GPU之间通信，基于nvidia/cuda devel版本 的镜像，应该都安装了nccl

```
whereis libnccl.so
# 查看nccl版本
python3 -c "import torch;print(torch.cuda.nccl.version())"
#(2, 18, 1)
```

如果不存在，则安装nccl：[Ubuntu NCCL安装 - 知乎](https://zhuanlan.zhihu.com/p/174710896)

### 4. **配置NCCL(非必须)**

**指定SOCKET网卡**

容器内运行ifconfig，找到节点ip对应的网卡，设置环境变量，写入/etc/profile中

**其他配置**，参考[大模型训练场景下NCCL环境变量设置 - 知乎](https://zhuanlan.zhihu.com/p/653001915)

```
export NCCL_SOCKET_IFNAME=bond0
export NCCL_PXN_DISABLE=1
export NCCL_DEBUG=info #方便调试
# 未使用IB，以下无需设置
export NCCL_IB_TIMEOUT=22
export NCCL_IB_RETRY_CNT=13
```

### 5. **启动训练**

[PyTorch Lightning入门教程（二） - 知乎](https://zhuanlan.zhihu.com/p/561011846)

[GPU training (Intermediate) &mdash; PyTorch Lightning 2.2.1 documentation](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html)

**config设置：**

参考config_gan_tiny_ddp.yaml

```
trainer:
  accelerator: gpu
  strategy:
    class_path: lightning.pytorch.strategies.ddp.DDPStrategy
    init_args:
      timeout: "00:10:00"
      find_unused_parameters: true

# 根据每个节点使用的GPU数，设置
  devices: [0]
# 根据机器数，设置节点数
  num_nodes: 2
```

**训练脚本run_ddp_train.sh**

```
#/bin/bash

MASTER_ADDR='172.16.12.2' # ainode02 as master
# MASTER_ADDR='172.16.12.8' # ainode08 as master
MASTER_PORT='25678' #根据主机上未占用的端口修改

GPUS=$1
RANK=$2
config_path=configs/config_gan_tiny_ddp.yaml

CUDA_VISIBLE_DEVICES=$GPUS MASTER_ADDR=$MASTER_ADDR MASTER_PORT=$MASTER_PORT WORLD_SIZE=2 NODE_RANK=$RANK LOCAL_RANK=0 \
python3 cli.py fit -c $config_path >logs/nohup_logs/nohup.gan.ddp.log 2>&1
```

**启动训练**

```
#主机上运行，并激活端口，会等待从机训练启动
./run_ddp_train.sh 0 0
#从机上运行
./run_ddp_train.sh 0 1
```

**关键日志：**

1. 确认所有节点都启动训练

```
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
```

2. NCCL已安装

```
ainode02:272019:272019 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.1+cuda12.1
```

3. 从机和主机之间NCCL通信正常

```
ainode08:2950111:2950111 [0] NCCL INFO cudaDriverVersion 12020
ainode08:2950111:2950111 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
ainode08:2950111:2950111 [0] NCCL INFO Bootstrap : Using bond0:172.16.12.8<0>
ainode08:2950111:2950111 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
ainode08:2950111:2950111 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
ainode08:2950111:2950934 [0] NCCL INFO Failed to open libibverbs.so[.1]
ainode08:2950111:2950934 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to bond0
ainode08:2950111:2950934 [0] NCCL INFO NET/Socket : Using [0]bond0:172.16.12.8<0>
ainode08:2950111:2950934 [0] NCCL INFO Using network Socket
ainode08:2950111:2950934 [0] NCCL INFO NCCL_PXN_DISABLE set by environment to 1.
...
ainode08:2950111:2950934 [0] NCCL INFO Connected all rings
ainode08:2950111:2950934 [0] NCCL INFO Connected all trees
ainode08:2950111:2950934 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
ainode08:2950111:2950934 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
ainode08:2950111:2950934 [0] NCCL INFO comm 0x55ef29c93970 rank 1 nranks 2 cudaDev 0 busId 34000 commId 0xc40540e13bf8d8f2 - Init COMPLETE
```

## 单机多卡（以NATSpeech训练为例）
### 一、背景

NATSpeech代码训练模型`CUDA_VISIBLE_DEVICES=0,1,2 python tasks/run.py --config xx --exp_name xx --reset `方式下，声学模型同样的配置下训练速度由单卡2~3 step/s变为 2~3 s/step，速度下降至单卡的6倍之多。那么为何多卡训练反而比单卡慢很多呢？

### 二、能跑通多卡的代码

看多卡训练正常的代码 train_infer_mgpu的代码，在单卡正常训练流程之外，针对多卡训练存在的操作有：

```python
from torch.distributed import init_process_group
# 设置多进程启动的方法
if args.num_gpus > 1:
    # random tcp port[54000, 54999] for multi-task in one machine
    rnd_port = 54000 + random.randint(0, 888)
    args.tcp_port = str(rnd_port)
    args.batch_size = int(args.batch_size / args.num_gpus)
    print("Batch size pre GPU :", args.batch_size)
    # train是训练的接口
    mp.spawn(train, nprocs=args.num_gpus, args=(args, json_config))
...
# 分布式初始化函数
init_process_group(backend='nccl', init_method='tcp://localhost:'+str(args.tcp_port), world_size=1*args.num_gpus, rank=rank)
...
# 模型部分更改，分布式并行模型
if args.num_gpus > 1:
    netG = DistributedDataParallel(netG, device_ids=[rank], find_unused_parameters=True).to(device)
    netD = DistributedDataParallel(netD, device_ids=[rank]).to(device)
    stftD = DistributedDataParallel(stftD, device_ids=[rank]).to(device)         
...
# 分布式加载数据集
# DDP：设置sampler的epoch，
    # DistributedSampler需要这个来指定shuffle方式，
    # 通过维持各个进程之间的相同随机数种子使不同进程能获得同样的shuffle效果。
# DDP：需要注意的是，这里的batch_size指的是每个进程下的batch_size。
	#  也就是说，总batch_size是这里的batch_size再乘以并行数(world_size)。
train_set = AudioDataset(mel_config, args.data_path, args.seq_len, load_mel=False, is_training=True)
train_sampler = DistributedSampler(train_set) if args.num_gpus > 1 else RandomSampler(train_set)
train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=3*args.num_gpus, sampler=train_sampler, drop_last=True)

if args.num_gpus > 1:
     train_sampler.set_epoch(epoch)
...
# 保存模型参数相关
# DDP:
# 1. save模型的时候，和DP模式一样，有一个需要注意的点：保存的是model.module而不是model。
	#  因为model其实是DDP model，参数是被`model=DDP(model)`包起来的。
# 2. 只需要在进程0上保存一次就行了，避免多次保存重复的东西。
	# if dist.get_rank() == 0:
	#     torch.save(model.module.state_dict(), "%d.ckpt" % epoch)
save_checkpoint(os.path.join(ckpt_dir, "model_{}.pt".format(steps+1)), {"state_dict_g": (netG.module if args.num_gpus > 1 else netG).state_dict()})
...
#
if steps % args.save_interval == 0 and steps != 0:
    save_checkpoint(os.path.join(ckpt_dir, "model_{}.pt".format(steps)),
        {
        "state_dict_g": (netG.module if args.num_gpus > 1 else netG).state_dict(),
        "state_dict_d": (netD.module if args.num_gpus > 1 else netD).state_dict(),
        "state_dict_stftD": (stftD.module if args.num_gpus > 1 else stftD).state_dict(),
        "optimizer_g": optG.state_dict(),
        "optimizer_d": optD.state_dict(),
        "steps": steps, 
        "epoch": epoch,
        }
    )
```

这里多卡训练的启动姿势如下：

```bash
CUDA_VISIBLE_DEVICES="0,1" python main.py
```

### 三、详解多卡训练每一步

1. 分布式训练启动

   方式一：

   ```bash
   python -m torch.distributed.launch --nproc_per_node=4 xxxxx main.py xxxxx
   ```

   方式二：

   ```bash
   import torch.multiprocessing as mp
   mp.spawn(target=main, nprocs=4, args=(xxx,xxx,...), join=True)
   ```

   torch.distributed.launch是通过python的subprocess来实现多进程，而mp.spawn是通过mp.Process来实现多进程，强制要求了start_method=spawn

   **区别**：spawn启动效率非常低，另外一个问题是运行时会序列化，如果有很大的对象[大于4G]会运行失败（第一种方式没有这一问题），优点在于接口比较友好，跨平台也比较方便

2. 分布式初始化方式

   

3. 分布式数据加载

dataloader部分没问题，DistributedSampler也是将数据按batch的去取，NATSpeech中是手动自己实现的

常用的分布式加载的代码如下

```python
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader
# dataset是数据集，对原始训练数据的封装，将其封装成python可识别的数据结构
# 采样器，采样方式或者是采样策略，实现从dataset中拿到数据索引，供dataloader使用
sampler = DistributedSampler(dataset) if is_distributed else None
# 负责依据索引来从数据集中加载数据，支持map-style和iterable-style两种dataset，支持单进程、多进程加载
loader = DataLoader(dataset, shuffle=(sampler is None), sampler=sampler)
for epoch in range(start_epoch, n_epochs):
	if is_distributed:
        sampler.set_epoch(epoch)
    train(loader)
```

loader依据indices加载数据，同时将数据发送给模型，进行训练。[Dataloader支持map-style dataset，A map-style dataset is one that implements the `__getitem__()` and `__len__()` protocols， 另外iterable-style是An iterable-style dataset is an instance of a subclass of [`IterableDataset`] that implements the `__iter__()` protocol](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset)

sampler在分布式中的重点是，如何让每个worker在数据集中只加载自己所属的部分，并且worker之间实现对数据集的正交分配。重点是DistributedSampler，下面详细解析这个class。

初始化方法中主要设置了本worker节点的各种信息，如数据集、rank(gpu的全局序号)、num_replicas副本数目。

> - dataset ：采样的数据集。
> - num_replicas ：参与分布式训练的进程数目，如未设置，则从group之中得到world_size作为进程数目。
> - rank : 当前进程的序号，如果没有设置，则从group之中得到。
> - shuffle ：采样是否需要打乱indices。
> - seed ：如果需要打乱，则设定一个random seed。
> - drop_last ：如果不能均匀分割数据，是否需要把无法分配的尾部数据丢掉。
> - epoch ：每次epoch都会shuffle数据集，如何保持shuffle之后数据集一致性？就是通过epoch完成。

在迭代时，DistributedSampler被实现成一个迭代器，实际上返回的时一个迭代器iter，核心的部分为返回indices, `indices = indices[self.rank:self.total_size:self.num_replicas]`

这里可以看到，对每个gpu（对应rank），都会取一段连续的num_replicas个数据。

但在训练过程中，每个epoch都会shuffle数据集，如何在不同进程中保证shuffle之后数据集的一致性？

在示例代码中有一行，`sampler.set_epoch(epoch)`就是起的这个作用。从源码中可以看到，epoch值能影响到random的seed值，从而保证不同进程中都使用同样的随机数种子。

以上代码在env/lib/python3.8/site-packages/torch/utils/data/distributed.py中

```python
import math
from typing import TypeVar, Optional, Iterator

import torch
from . import Sampler, Dataset
import torch.distributed as dist


T_co = TypeVar('T_co', covariant=True)


class DistributedSampler(Sampler[T_co]):
    r"""Sampler that restricts data loading to a subset of the dataset.

    It is especially useful in conjunction with
    :class:`torch.nn.parallel.DistributedDataParallel`. In such a case, each
    process can pass a :class:`~torch.utils.data.DistributedSampler` instance as a
    :class:`~torch.utils.data.DataLoader` sampler, and load a subset of the
    original dataset that is exclusive to it.

    .. note::
        Dataset is assumed to be of constant size and that any instance of it always
        returns the same elements in the same order.

    Args:
        dataset: Dataset used for sampling.
        num_replicas (int, optional): Number of processes participating in
            distributed training. By default, :attr:`world_size` is retrieved from the
            current distributed group.
        rank (int, optional): Rank of the current process within :attr:`num_replicas`.
            By default, :attr:`rank` is retrieved from the current distributed
            group.
        shuffle (bool, optional): If ``True`` (default), sampler will shuffle the
            indices.
        seed (int, optional): random seed used to shuffle the sampler if
            :attr:`shuffle=True`. This number should be identical across all
            processes in the distributed group. Default: ``0``.
        drop_last (bool, optional): if ``True``, then the sampler will drop the
            tail of the data to make it evenly divisible across the number of
            replicas. If ``False``, the sampler will add extra indices to make
            the data evenly divisible across the replicas. Default: ``False``.

    .. warning::
        In distributed mode, calling the :meth:`set_epoch` method at
        the beginning of each epoch **before** creating the :class:`DataLoader` iterator
        is necessary to make shuffling work properly across multiple epochs. Otherwise,
        the same ordering will be always used.

    Example::

        >>> sampler = DistributedSampler(dataset) if is_distributed else None
        >>> loader = DataLoader(dataset, shuffle=(sampler is None),
        ...                     sampler=sampler)
        >>> for epoch in range(start_epoch, n_epochs):
        ...     if is_distributed:
        ...         sampler.set_epoch(epoch)
        ...     train(loader)
    """

    def __init__(self, dataset: Dataset, num_replicas: Optional[int] = None,
                 rank: Optional[int] = None, shuffle: bool = True,
                 seed: int = 0, drop_last: bool = False) -> None:
        if num_replicas is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            num_replicas = dist.get_world_size()
        if rank is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            rank = dist.get_rank()
        if rank >= num_replicas or rank < 0:
            raise ValueError(
                "Invalid rank {}, rank should be in the interval"
                " [0, {}]".format(rank, num_replicas - 1))
        self.dataset = dataset
        self.num_replicas = num_replicas
        self.rank = rank
        self.epoch = 0
        self.drop_last = drop_last
        # If the dataset length is evenly divisible by # of replicas, then there
        # is no need to drop any data, since the dataset will be split equally.
        if self.drop_last and len(self.dataset) % self.num_replicas != 0:  # type: ignore[arg-type]
            # Split to nearest available length that is evenly divisible.
            # This is to ensure each rank receives the same amount of data when
            # using this Sampler.
            self.num_samples = math.ceil(
                (len(self.dataset) - self.num_replicas) / self.num_replicas  # type: ignore[arg-type]
            )
        else:
            self.num_samples = math.ceil(len(self.dataset) / self.num_replicas)  # type: ignore[arg-type]
        self.total_size = self.num_samples * self.num_replicas
        self.shuffle = shuffle
        self.seed = seed

    def __iter__(self) -> Iterator[T_co]:
        if self.shuffle:
            # deterministically shuffle based on epoch and seed
            g = torch.Generator()
            g.manual_seed(self.seed + self.epoch)
            indices = torch.randperm(len(self.dataset), generator=g).tolist()  # type: ignore[arg-type]
        else:
            indices = list(range(len(self.dataset)))  # type: ignore[arg-type]

        if not self.drop_last:
            # add extra samples to make it evenly divisible
            padding_size = self.total_size - len(indices)
            if padding_size <= len(indices):
                indices += indices[:padding_size]
            else:
                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]
        else:
            # remove tail of data to make it evenly divisible.
            indices = indices[:self.total_size]
        assert len(indices) == self.total_size

        # subsample
        indices = indices[self.rank:self.total_size:self.num_replicas]
        assert len(indices) == self.num_samples

        return iter(indices)

    def __len__(self) -> int:
        return self.num_samples

    def set_epoch(self, epoch: int) -> None:
        r"""
        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas
        use a different random ordering for each epoch. Otherwise, the next iteration of this
        sampler will yield the same ordering.

        Args:
            epoch (int): Epoch number.
        """
        self.epoch = epoch
```

### 四、常见问题

#### 1. 多GPU 负载不均衡

多卡训练时，只有主卡占了显存，其他卡几乎没占，负载不均衡到不能忍，参考[这里](https://discuss.pytorch.org/t/dataparallel-imbalanced-memory-usage/22551/20)

可能计算loss占了大量的内存，如果每个GPU分别计算loss，然后将loss返回到主卡上

这样操作之后，主GPU 会稍微高一点。多个几百M, 但是基本实现了负载均衡
