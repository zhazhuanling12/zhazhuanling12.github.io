## **总结**

- **.ckpt**：训练态**快照**（含优化器等完整状态，便于断点恢复）。
- **.safetensors**：**安全权重**（仅参数，加载快、可分片、跨语言友好）。
- **.jit（TorchScript）**：PyTorch 生态内的**可执行计算图**（C++/TorchServe 直接推理）。
- **.onnx**：跨生态的**通用计算图**（ONNXRuntime / TensorRT / OpenVINO 等后端）。

## **生命周期视图**

```
[训练中]                 [训练完成]
.ckpt  ──►  .safetensors  ──►  .jit / .onnx
   │                 │                 │
   │                 │                 └─► 硬件编译后部署（TensorRT/OpenVINO/TFLite/CoreML…）
   └─────► 恢复训练 / 微调
```


## **核心对比**

| **格式** | **用途定位** | **依赖环境** | **仅存权重** | **可直接推理** | **典型场景** |
| --- | --- | --- | --- | --- | --- |
| **.ckpt / .bin** | 训练断点恢复 | PyTorch | 否（含优化器等） | 否 | 训练中间保存/恢复 |
| **.safetensors** | 安全权重保存 | PyTorch / Transformers | 是 | 需加载到模型类 | 推理前权重发布 |
| **.jit (TorchScript)** | 可序列化执行图 | PyTorch Runtime | 是 | ✅ | C++/TorchServe 内部部署 |
| **.onnx** | 跨框架通用图 | 任意 ONNX Runtime | 是 | ✅ | 工业化部署（TensorRT/ONNXRuntime/OpenVINO） |

---

## **训练态格式**

### **.ckpt / .bin**

- 含 model.state_dict() + 训练状态（optimizer、scheduler、随机种子等）。
- 基于 torch.save()（pickle 序列化）。
- 优点：可完整恢复训练。缺点：安全性与加载速度一般。

### **.safetensors**

- Hugging Face 推出的安全权重格式（无 pickle，可 mmap 零拷加载，拆分分片）。
- 优点：**快**（常 30–50% 更快加载）、**安全**、**可分片**、**跨语言**；
- 注意：不包含优化器状态（不用于恢复训练）。

---

## **部署态格式**

### TorchScript（.jit / .ptl）

- 生成方式：trace（快、对动态控制流不友好）或 script（稳、支持控制流）。
- 适合：PyTorch 内部生态部署（C++/TorchServe），消除 Python 调度开销。

### ONNX（.onnx）

- 跨框架标准中间层。
- 适合：ONNXRuntime / TensorRT / OpenVINO 进一步图优化、量化与编译。
- 限制：仅前向图；自定义算子需实现导出/替换。

---

## **推理优化与量化格式（编译后产物）**

- **TensorRT Engine（.engine）**：NVIDIA GPU 上的编译可执行；极致性能，硬件/驱动绑定。
- **OpenVINO IR（.xml+.bin）**：Intel 平台高性能推理。
- **TFLite（.tflite）/ CoreML（.mlmodel）**：移动与端侧。
- **GGUF / GGML / AWQ / GPTQ**：LLM 轻量推理与低比特量化的事实标准（本地/Ollama/llama.cpp）。

---

## **分布式训练下的存储策略**

### **模式与存储语义**

| **并行/切分** | **权重是否分片** | **保存对象** | **典型格式** | **恢复难度** |
| --- | --- | --- | --- | --- |
| **DDP** | 否 | 全量权重 + 优化器 | .ckpt/.bin/.safetensors | 低 |
| **ZeRO-1/2（DeepSpeed）** | 权重不分片（分片优化器/梯度） | 分片状态 + 可选全量权重 | .pt +（rank0 导出）.safetensors | 中 |
| **ZeRO-3 / FSDP** | ✅ 权重分片 | 各 rank 分片 +（可选合并导出） | 多分片 .pt / 分片 .safetensors+index.json | 中-高 |
| **Megatron（TP/PP）** | ✅ 张量/流水线分片 | 每 rank/每 stage 片段 + index.json | 分片 .safetensors | 中 |

### **结论**

- **ZeRO-2**：保存 .safetensors 基本不需要跨 rank 合并（权重未分片），影响较小；主要关注 I/O 与 barrier。
- **ZeRO-3/FSDP**：保存单一 .safetensors 需要 **all-gather 合并至 rank0**，**易触发通信/I-O 超时**。
- 稳妥做法：**训练阶段仅存分片 checkpoint（.pt）**；**训练后离线合并并导出 .safetensors**，或**直接使用分布式分片 .safetensors + index.json**（新趋势）。

---

## **不同格式所属阶段**

1. **训练中**：按并行策略保存 **可恢复 checkpoint**（DeepSpeed/FSDP 原生命令，分片 .pt）。
2. **评估/发布**：在 **rank0 或离线** 从 checkpoint 合并并导出 **.safetensors**（max_shard_size 控制单文件大小）。
3. **部署准备**：
    - **PyTorch 生态**：导出 **TorchScript**。
    - **跨生态/高性能**：导出 **ONNX**，再编译 **TensorRT/OpenVINO**。
    - **端侧/本地 LLM**：转换 **GGUF/GGML** 或 TFLite/CoreML。
4. **分布式大模型趋势**：直接产出 **分片 .safetensors + index.json**，避免全量合并与 NCCL 超时。

---

## **常见风险与规避要点**

- **训练中导出全量 .safetensors（ZeRO-3/FSDP）**：会触发全模型 all-gather → 可能 NCCL 超时。
    - 规避：仅 rank0 导出；移至训练后离线导出；或使用分片 safetensors。
- **I/O 风暴**：save_on_each_node=True/共享文件系统拥堵。
    - 规避：仅 rank0 写本地 NVMe，异步上传；分层目录避免元数据锁竞争。
- **并行叠加（TP/PP+ZeRO）**：确保以 index.json 正确标注分片映射；统一导入/导出脚本。

---

## **结论**

> 训练态用 .ckpt/.pt 做“可恢复快照”；发布态用 .safetensors 做“安全权重”；部署态按场景选择 .jit（PyTorch 内）或 .onnx（跨生态），并在需要时编译为 .engine/.tflite/.mlmodel/ GGUF 等硬件优化格式。
> 

> 分布式训练下，优先分片存储 + 离线合并；或采用分片 .safetensors + index.json 以避免全量 merge 带来的通信与 I/O 超时。
